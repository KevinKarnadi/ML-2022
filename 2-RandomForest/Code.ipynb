{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRo67Io4NKF"
      },
      "source": [
        "# **Decision Tree and Random Forest**\n",
        "\n",
        "1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes\n",
        "> * Step 1 : Load the input data\n",
        "> * Step 2 : Calculate the Entropy and Information Gain\n",
        "> * Step 3 : Find the Best Split\n",
        "> * Step 4 : Split into 2 branches\n",
        "> * Step 5 : Build decision tree\n",
        "> * Step 6 : Split data into training set and validation set\n",
        "> * Step 7 : Train a decision tree model with training set\n",
        "> * Step 8 : Predict the cases in the *validation set* by using the model trained in *Step7*\n",
        "> * Step 9 : Calculate the f1-score of your predictions in *Step8*\n",
        "\n",
        "2. Advanced Part : Build a **Random Forest** model to make predictions\n",
        "> * Step 1 : Load the input data\n",
        "> * Step 2 : Load the test data\n",
        "> * Step 3 : Build a random forest\n",
        "> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wwVh8lYD4kbV"
      },
      "source": [
        "# **Basic Part**\n",
        "\n",
        "Implement a Decision Tree model by completing the following functions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h2ibEyDa46X2"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RMjaYVZD6kmb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zrQXqH475G8-"
      },
      "source": [
        "## Step1: Load the input data\n",
        "\n",
        "First, load the input file **input_basic.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "0n3gcL2l6kjb",
        "outputId": "21fda5c9-f6de-4d43-f71f-174009bb1653"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>heart_rate_apache</th>\n",
              "      <th>resprate_apache</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.0</td>\n",
              "      <td>25.984659</td>\n",
              "      <td>1</td>\n",
              "      <td>172.7</td>\n",
              "      <td>77.50</td>\n",
              "      <td>116.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>31.310368</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>90.70</td>\n",
              "      <td>71.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54.0</td>\n",
              "      <td>24.388824</td>\n",
              "      <td>1</td>\n",
              "      <td>177.8</td>\n",
              "      <td>77.10</td>\n",
              "      <td>120.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>65.0</td>\n",
              "      <td>34.141074</td>\n",
              "      <td>0</td>\n",
              "      <td>170.2</td>\n",
              "      <td>98.90</td>\n",
              "      <td>73.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49.0</td>\n",
              "      <td>22.564743</td>\n",
              "      <td>1</td>\n",
              "      <td>172.7</td>\n",
              "      <td>67.30</td>\n",
              "      <td>207.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>62.0</td>\n",
              "      <td>29.424010</td>\n",
              "      <td>0</td>\n",
              "      <td>154.9</td>\n",
              "      <td>70.60</td>\n",
              "      <td>113.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>85.0</td>\n",
              "      <td>27.673574</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>66.40</td>\n",
              "      <td>102.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>65.0</td>\n",
              "      <td>22.269432</td>\n",
              "      <td>1</td>\n",
              "      <td>177.8</td>\n",
              "      <td>70.40</td>\n",
              "      <td>333.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>85.0</td>\n",
              "      <td>35.879362</td>\n",
              "      <td>0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>97.80</td>\n",
              "      <td>124.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>81.0</td>\n",
              "      <td>20.859375</td>\n",
              "      <td>0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>53.40</td>\n",
              "      <td>136.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>59.0</td>\n",
              "      <td>46.409136</td>\n",
              "      <td>0</td>\n",
              "      <td>162.6</td>\n",
              "      <td>122.70</td>\n",
              "      <td>169.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>77.0</td>\n",
              "      <td>32.324734</td>\n",
              "      <td>0</td>\n",
              "      <td>154.9</td>\n",
              "      <td>77.56</td>\n",
              "      <td>264.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>68.0</td>\n",
              "      <td>15.913579</td>\n",
              "      <td>1</td>\n",
              "      <td>185.4</td>\n",
              "      <td>54.70</td>\n",
              "      <td>39.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>51.0</td>\n",
              "      <td>24.028492</td>\n",
              "      <td>1</td>\n",
              "      <td>190.5</td>\n",
              "      <td>87.20</td>\n",
              "      <td>80.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>76.0</td>\n",
              "      <td>34.216873</td>\n",
              "      <td>0</td>\n",
              "      <td>154.9</td>\n",
              "      <td>82.10</td>\n",
              "      <td>306.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>48.0</td>\n",
              "      <td>26.516476</td>\n",
              "      <td>1</td>\n",
              "      <td>180.3</td>\n",
              "      <td>86.20</td>\n",
              "      <td>96.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>82.0</td>\n",
              "      <td>18.921389</td>\n",
              "      <td>0</td>\n",
              "      <td>154.9</td>\n",
              "      <td>45.40</td>\n",
              "      <td>164.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>78.0</td>\n",
              "      <td>36.668167</td>\n",
              "      <td>0</td>\n",
              "      <td>167.6</td>\n",
              "      <td>103.00</td>\n",
              "      <td>282.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>62.0</td>\n",
              "      <td>19.108088</td>\n",
              "      <td>1</td>\n",
              "      <td>157.5</td>\n",
              "      <td>47.40</td>\n",
              "      <td>275.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>73.0</td>\n",
              "      <td>22.851562</td>\n",
              "      <td>0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>58.50</td>\n",
              "      <td>178.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>73.0</td>\n",
              "      <td>20.351971</td>\n",
              "      <td>0</td>\n",
              "      <td>167.5</td>\n",
              "      <td>57.10</td>\n",
              "      <td>159.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>59.0</td>\n",
              "      <td>27.109238</td>\n",
              "      <td>1</td>\n",
              "      <td>177.8</td>\n",
              "      <td>85.70</td>\n",
              "      <td>85.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>51.0</td>\n",
              "      <td>37.954367</td>\n",
              "      <td>0</td>\n",
              "      <td>172.7</td>\n",
              "      <td>113.20</td>\n",
              "      <td>168.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>80.0</td>\n",
              "      <td>36.267895</td>\n",
              "      <td>1</td>\n",
              "      <td>180.3</td>\n",
              "      <td>117.90</td>\n",
              "      <td>138.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>74.0</td>\n",
              "      <td>25.856081</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>74.90</td>\n",
              "      <td>145.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>61.0</td>\n",
              "      <td>29.161972</td>\n",
              "      <td>1</td>\n",
              "      <td>180.3</td>\n",
              "      <td>94.80</td>\n",
              "      <td>267.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>74.0</td>\n",
              "      <td>38.111136</td>\n",
              "      <td>1</td>\n",
              "      <td>188.0</td>\n",
              "      <td>134.70</td>\n",
              "      <td>279.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>65.0</td>\n",
              "      <td>53.414791</td>\n",
              "      <td>0</td>\n",
              "      <td>166.4</td>\n",
              "      <td>147.90</td>\n",
              "      <td>121.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>38.0</td>\n",
              "      <td>33.438787</td>\n",
              "      <td>0</td>\n",
              "      <td>161.3</td>\n",
              "      <td>87.00</td>\n",
              "      <td>135.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>82.0</td>\n",
              "      <td>32.263238</td>\n",
              "      <td>0</td>\n",
              "      <td>162.6</td>\n",
              "      <td>85.30</td>\n",
              "      <td>111.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  glucose_apache  \\\n",
              "0   70.0  25.984659       1   172.7   77.50           116.0   \n",
              "1   30.0  31.310368       1   170.2   90.70            71.0   \n",
              "2   54.0  24.388824       1   177.8   77.10           120.0   \n",
              "3   65.0  34.141074       0   170.2   98.90            73.0   \n",
              "4   49.0  22.564743       1   172.7   67.30           207.0   \n",
              "5   62.0  29.424010       0   154.9   70.60           113.0   \n",
              "6   85.0  27.673574       1   154.9   66.40           102.0   \n",
              "7   65.0  22.269432       1   177.8   70.40           333.0   \n",
              "8   85.0  35.879362       0   165.1   97.80           124.0   \n",
              "9   81.0  20.859375       0   160.0   53.40           136.0   \n",
              "10  59.0  46.409136       0   162.6  122.70           169.0   \n",
              "11  77.0  32.324734       0   154.9   77.56           264.0   \n",
              "12  68.0  15.913579       1   185.4   54.70            39.0   \n",
              "13  51.0  24.028492       1   190.5   87.20            80.0   \n",
              "14  76.0  34.216873       0   154.9   82.10           306.0   \n",
              "15  48.0  26.516476       1   180.3   86.20            96.0   \n",
              "16  82.0  18.921389       0   154.9   45.40           164.0   \n",
              "17  78.0  36.668167       0   167.6  103.00           282.0   \n",
              "18  62.0  19.108088       1   157.5   47.40           275.0   \n",
              "19  73.0  22.851562       0   160.0   58.50           178.0   \n",
              "20  73.0  20.351971       0   167.5   57.10           159.0   \n",
              "21  59.0  27.109238       1   177.8   85.70            85.0   \n",
              "22  51.0  37.954367       0   172.7  113.20           168.0   \n",
              "23  80.0  36.267895       1   180.3  117.90           138.0   \n",
              "24  74.0  25.856081       1   170.2   74.90           145.0   \n",
              "25  61.0  29.161972       1   180.3   94.80           267.0   \n",
              "26  74.0  38.111136       1   188.0  134.70           279.0   \n",
              "27  65.0  53.414791       0   166.4  147.90           121.0   \n",
              "28  38.0  33.438787       0   161.3   87.00           135.0   \n",
              "29  82.0  32.263238       0   162.6   85.30           111.0   \n",
              "\n",
              "    heart_rate_apache  resprate_apache  sodium_apache  diabetes_mellitus  \n",
              "0               101.0             49.0          137.0                  0  \n",
              "1                39.0             33.0          144.0                  0  \n",
              "2               120.0             31.0          141.0                  0  \n",
              "3                48.0             36.0          140.0                  1  \n",
              "4               119.0              6.0          144.0                  0  \n",
              "5                60.0             32.0          137.0                  0  \n",
              "6                49.0             36.0          142.0                  0  \n",
              "7                59.0              6.0          145.0                  1  \n",
              "8                92.0             30.0          136.0                  0  \n",
              "9               118.0             52.0          138.0                  0  \n",
              "10              100.0             46.0          138.0                  0  \n",
              "11               90.0             37.0          141.0                  1  \n",
              "12              108.0             45.0          135.0                  0  \n",
              "13               61.0             30.0          139.0                  0  \n",
              "14              112.0             40.0          130.0                  1  \n",
              "15              133.0             31.0          137.0                  0  \n",
              "16              103.0             48.0          134.0                  0  \n",
              "17              104.0             42.0          138.0                  1  \n",
              "18              108.0             38.0          131.0                  1  \n",
              "19              154.0             55.0          138.0                  1  \n",
              "20              110.0             27.0          139.0                  0  \n",
              "21               92.0              6.0          143.0                  0  \n",
              "22               60.0             43.0          119.0                  1  \n",
              "23              178.0             43.0          140.0                  1  \n",
              "24               40.0             16.0          146.0                  1  \n",
              "25               62.0             58.0          134.0                  1  \n",
              "26              112.0             43.0          132.0                  1  \n",
              "27               88.0             32.0          142.0                  0  \n",
              "28               46.0             28.0          138.0                  0  \n",
              "29              114.0             22.0          143.0                  1  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data = pd.read_csv('input_basic.csv')\n",
        "input_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BhtqUTG9Nlyz"
      },
      "source": [
        "## Global attributes\n",
        "\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "etfPC94oN_TO"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = input_data.shape[1] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KQ-OYop8ONnv"
      },
      "outputs": [],
      "source": [
        "mode = \"basic\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gey7t_Yx5YML"
      },
      "source": [
        "## Step2 : Calculate the Entropy and Information Gain\n",
        "\n",
        "Calculate the information gain and entropy values before separate data into left subtree and right subtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpdNz3ij6keH",
        "outputId": "c6df8ef7-c8c5-4890-fc37-2a70e660948d"
      },
      "outputs": [],
      "source": [
        "from logging import log\n",
        "def entropy(data):\n",
        "  \"\"\"\n",
        "  This function measures the amount of uncertainty in a probability distribution\n",
        "  args: \n",
        "  * data(type: DataFrame): the data you're calculating for the entropy\n",
        "  return:\n",
        "  * entropy_value(type: float): the data's entropy\n",
        "  \"\"\"\n",
        "  \n",
        "  if data.empty:\n",
        "    return 0\n",
        "\n",
        "  # Count values\n",
        "  p = list(data[\"diabetes_mellitus\"]).count(1)\n",
        "  n = list(data[\"diabetes_mellitus\"]).count(0)\n",
        "  \n",
        "  p_positive = p/(p+n)\n",
        "  p_negative = n/(p+n)\n",
        "  if p_positive == 0 or p_negative == 0:\n",
        "    entropy_value = 0\n",
        "  else:\n",
        "    entropy_value = -p_positive*math.log2(p_positive) - p_negative*math.log2(p_negative)\n",
        "  \n",
        "  return entropy_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCC_SiU26kbX",
        "outputId": "ce08995a-b6e9-4739-c9bc-2762d62dacc0"
      },
      "outputs": [],
      "source": [
        "def information_gain(data, mask):\n",
        "  \"\"\"\n",
        "  This function will calculate the information gain\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you're calculating for the information gain\n",
        "  * mask(type: Series): partition information(left/right) of current input data, \n",
        "    - boolean 1(True) represents split to left subtree\n",
        "    - boolean 0(False) represents split to right subtree\n",
        "  return:\n",
        "  * ig(type: float): the information gain you can obtain by classify data with this given mask\n",
        "  \"\"\"\n",
        "\n",
        "  left = data[mask]\n",
        "  right = data[~mask]\n",
        "  \n",
        "  w_left = len(left) / (len(left) + len(right))\n",
        "  w_right = len(right) / (len(left) + len(right))\n",
        "  ig = entropy(data) - (w_left*entropy(left) + w_right*entropy(right))\n",
        "\n",
        "  return ig"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9r8mrn7A55if"
      },
      "source": [
        "## Step3 : Find the Best Split\n",
        "\n",
        "Find the best split combination, **feature** and **threshold**, by calculating the information gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6gg7ig18XgM",
        "outputId": "c21c70aa-69fa-46ec-a9e1-8b9c2df8e96b"
      },
      "outputs": [],
      "source": [
        "def find_best_split(data):\n",
        "  \"\"\"\n",
        "  This function will find the best split combination of data\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  return\n",
        "  * best_ig(type: float): the best information gain you obtain\n",
        "  * best_threshold(type: float): the value that splits data into 2 branches\n",
        "  * best_feature(type: string): the feature that splits data into 2 branches\n",
        "  \"\"\"\n",
        "\n",
        "  best_ig = 0\n",
        "  best_threshold = 0\n",
        "  best_feature = \"\"\n",
        "  for feature in data.loc[:, data.columns != \"diabetes_mellitus\"]:\n",
        "    temp_data = data.sort_values(feature, ignore_index=True)\n",
        "    unique_values = temp_data[feature].unique()\n",
        "    prev = None\n",
        "    for i in unique_values:\n",
        "      if prev != None:\n",
        "        midpoint = (prev + i) / 2\n",
        "        mask = (temp_data[feature] <= midpoint)\n",
        "        ig = information_gain(temp_data, mask)\n",
        "        if ig > best_ig:\n",
        "          best_ig = ig\n",
        "          best_threshold = midpoint\n",
        "          best_feature = feature\n",
        "      prev = i\n",
        "  \n",
        "  return best_ig, best_threshold, best_feature"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "61hPUYvy6MTB"
      },
      "source": [
        "## Step4 : Split into 2 branches\n",
        "\n",
        "Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQRcjzCLCo4R",
        "outputId": "e1661976-3d4b-4308-e551-94fabab9584e"
      },
      "outputs": [],
      "source": [
        "def make_partition(data, feature, threshold):\n",
        "  \"\"\"\n",
        "  This function will split the data into 2 branches\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  * feature(type: string): the attribute(column name)\n",
        "  * threshold(type: float): the threshold for splitting the data\n",
        "  return:\n",
        "  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n",
        "  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n",
        "  \"\"\"\n",
        "  \n",
        "  left = data.loc[data[feature] <= threshold]\n",
        "  right = data.loc[data[feature] > threshold]\n",
        "  \n",
        "  return left, right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLzy6Yhg802x"
      },
      "source": [
        "## Step5 : Build Decision Tree\n",
        "Use the above functions to implement the decision tree\n",
        "\n",
        "Instructions: \n",
        "1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n",
        "2.  Use function *find_best_split()* to find the best split combination\n",
        "3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n",
        "4. Use function *make_partition()* to split the data into two parts\n",
        "5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_OAXVddKkvM2"
      },
      "outputs": [],
      "source": [
        "def build_tree(data, max_depth, min_samples_split, depth):\n",
        "  \"\"\"\n",
        "  This function will build the decision tree\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you want to apply to the decision tree\n",
        "  * max_depth: the maximum depth of a decision tree\n",
        "  * min_samples_split: the minimum number of instances required to do partition\n",
        "  * depth: the height of the current decision tree\n",
        "  return:\n",
        "  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n",
        "  \"\"\"\n",
        "\n",
        "  temp_data = data.copy()\n",
        "  if mode == \"advanced\":\n",
        "    temp = temp_data[\"diabetes_mellitus\"]\n",
        "    temp_data = temp_data.drop([\"diabetes_mellitus\"], axis = 1)\n",
        "    temp_data = temp_data.sample(n = n_features, axis = 1, replace = False)\n",
        "    temp_data[\"diabetes_mellitus\"] = temp\n",
        "\n",
        "  # check the condition of current depth and the remaining number of samples\n",
        "  if depth < max_depth and len(data) > min_samples_split:\n",
        "\n",
        "    # call find_best_split() to find the best combination\n",
        "    ig, threshold, feature = find_best_split(temp_data)\n",
        "\n",
        "    # check the value of information gain is greater than 0 or not \n",
        "    if ig > 0 :\n",
        "      # update the depth\n",
        "      depth = depth + 1\n",
        "\n",
        "      # call make_partition() to split the data into two parts\n",
        "      left, right = make_partition(data, feature, threshold)\n",
        "\n",
        "      # If there is no data split to the left tree OR no data split to the left tree\n",
        "      if len(left) == 0 or len(right) == 0:\n",
        "\n",
        "        # return the label of the majority\n",
        "        p = list(data[\"diabetes_mellitus\"]).count(1)\n",
        "        n = list(data[\"diabetes_mellitus\"]).count(0)\n",
        "        label = 1 if p >= n else 0\n",
        "        return label\n",
        "      else:\n",
        "        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n",
        "        subtree = {question: []}\n",
        "\n",
        "        # call function build_tree() to recursively build the left subtree and right subtree\n",
        "        left_subtree = build_tree(left, max_depth, min_samples_split, depth)\n",
        "        right_subtree = build_tree(right, max_depth, min_samples_split, depth)\n",
        "\n",
        "        if left_subtree == right_subtree:\n",
        "          subtree = left_subtree\n",
        "        else:\n",
        "          subtree[question].append(left_subtree)\n",
        "          subtree[question].append(right_subtree)\n",
        "    else:\n",
        "      # return the label of the majority\n",
        "      p = list(data[\"diabetes_mellitus\"]).count(1)\n",
        "      n = list(data[\"diabetes_mellitus\"]).count(0)\n",
        "      label = 1 if p >= n else 0\n",
        "      return label\n",
        "  else:\n",
        "    # return the label of the majority\n",
        "    p = list(data[\"diabetes_mellitus\"]).count(1)\n",
        "    n = list(data[\"diabetes_mellitus\"]).count(0)\n",
        "    label = 1 if p >= n else 0\n",
        "    return label\n",
        "\n",
        "  return subtree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qlIrw9Gu-M9-"
      },
      "source": [
        "An example of the output from *build_tree()* \n",
        "```\n",
        "{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n",
        "```\n",
        "Therefore, \n",
        "```\n",
        "ans_features = ['bmi', 'age']\n",
        "ans_thresholds = [33.5, 68.5]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW8wm1rD9dlS",
        "outputId": "95bd8d94-1a7f-42be-dbac-3a5beab82db0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'glucose_apache <= 235.5': [{'heart_rate_apache <= 143.5': [0, 1]}, 1]}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n",
        "decisionTree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7DotyrSZjYKi"
      },
      "source": [
        "## Step6 : Split data\n",
        "\n",
        "Split data into training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjNM-n4i5mlG",
        "outputId": "e4c5a944-648e-46b0-ddc1-5c50edfcf680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 10)\n",
            "(20, 10)\n",
            "(10, 10)\n"
          ]
        }
      ],
      "source": [
        "num_train = 20\n",
        "num_validation = 10\n",
        "\n",
        "training_data = input_data.iloc[:num_train]\n",
        "validation_data = input_data.iloc[-num_validation:]\n",
        "\n",
        "y_train = training_data[[\"diabetes_mellitus\"]]\n",
        "x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n",
        "y_validation = validation_data[[\"diabetes_mellitus\"]]\n",
        "x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n",
        "y_validation = y_validation.values.flatten()\n",
        "\n",
        "print(input_data.shape)\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GfKSt2gH74Uu"
      },
      "source": [
        "## Step7 to Step9 : Make predictions with a decision tree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BZqSVoJ48a3-"
      },
      "source": [
        "Define the attributions of the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vSlZ7FVB8eau"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = x_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0piZ0blpFXVq"
      },
      "outputs": [],
      "source": [
        "def classify_data(instance, tree):\n",
        "  \"\"\"\n",
        "  This function will predict/classify the input instance\n",
        "  args:\n",
        "  * instance: a instance(case) to be predicted\n",
        "  return:\n",
        "  * answer: the prediction result (the classification result)\n",
        "  \"\"\"\n",
        "\n",
        "  equation = list(tree.keys())[0] \n",
        "  if equation.split()[1] == '<=':\n",
        "    temp_feature = equation.split()[0]\n",
        "    temp_threshold = equation.split()[2]\n",
        "    if instance[temp_feature] > float(temp_threshold):\n",
        "      answer = tree[equation][1]\n",
        "    else:\n",
        "      answer = tree[equation][0]\n",
        "  else:\n",
        "    if instance[equation.split()[0]] in (equation.split()[2]):\n",
        "      answer = tree[equation][0]\n",
        "    else:\n",
        "      answer = tree[equation][1]\n",
        "\n",
        "  if not isinstance(answer, dict):\n",
        "    return answer\n",
        "  else:\n",
        "    return classify_data(instance, answer)\n",
        "\n",
        "\n",
        "def make_prediction(tree, data):\n",
        "  \"\"\"\n",
        "  This function will use your pre-trained decision tree to predict the labels of all instances in data\n",
        "  args:\n",
        "  * tree: the decision tree\n",
        "  * data: the data to predict\n",
        "  return:\n",
        "  * y_prediction: the predictions\n",
        "  \"\"\"\n",
        "  \n",
        "  # [Note] You can call the function classify_data() to predict the label of each instance\n",
        "  y_prediction = []\n",
        "  for i in range(len(data)):\n",
        "    y_prediction.append(classify_data(data.iloc[i], tree))\n",
        "\n",
        "  return y_prediction\n",
        "\n",
        "\n",
        "def calculate_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  This function will calculate the f1-score of the predictions\n",
        "  args:\n",
        "  * y_true: the ground truth\n",
        "  * y_pred: the predictions\n",
        "  return:\n",
        "  * score: the f1-score\n",
        "  \"\"\"\n",
        "  \n",
        "  score = f1_score(y_true, y_pred)\n",
        "  \n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IEu3z3s9TDu",
        "outputId": "73499410-163c-4170-839b-68091b263138"
      },
      "outputs": [],
      "source": [
        "decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n",
        "\n",
        "y_pred = make_prediction(decision_tree, x_validation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tV25IjM7_aEn"
      },
      "source": [
        "# **Advanced Part**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "knH1Ih0Pha7X"
      },
      "source": [
        "## Step1: Load the input data\n",
        "\n",
        "First, load the input file **input_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FthBdLxRhi9W"
      },
      "outputs": [],
      "source": [
        "advanced_data = pd.read_csv('input_advanced.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vqLH49oBndRh"
      },
      "source": [
        "You can split *advanced_data* into training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9l0hLPVjncam"
      },
      "outputs": [],
      "source": [
        "num_train = round(len(advanced_data)*0.75)\n",
        "num_validation = round(len(advanced_data)*0.25)\n",
        "\n",
        "training_data = advanced_data.iloc[:num_train]\n",
        "validation_data = advanced_data.iloc[-num_validation:]\n",
        "\n",
        "y_train = training_data[[\"diabetes_mellitus\"]]\n",
        "x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n",
        "y_validation = validation_data[[\"diabetes_mellitus\"]]\n",
        "x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n",
        "y_validation = y_validation.values.flatten()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tFgbUY_ajVOK"
      },
      "source": [
        "## Step2 : Load the test data\n",
        "Load the input file **input_test.csv** to make predictions with the pre-trained random forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3hW542KWNxVF",
        "outputId": "1907d355-fef2-459d-8946-7053eb1929f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>arf_apache</th>\n",
              "      <th>bun_apache</th>\n",
              "      <th>creatinine_apache</th>\n",
              "      <th>gcs_eyes_apache</th>\n",
              "      <th>gcs_motor_apache</th>\n",
              "      <th>...</th>\n",
              "      <th>hematocrit_apache</th>\n",
              "      <th>intubated_apache</th>\n",
              "      <th>map_apache</th>\n",
              "      <th>resprate_apache</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>apache_4a_icu_death_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>62</td>\n",
              "      <td>32.866392</td>\n",
              "      <td>1</td>\n",
              "      <td>177.80</td>\n",
              "      <td>103.9</td>\n",
              "      <td>1</td>\n",
              "      <td>31.0</td>\n",
              "      <td>10.30</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0</td>\n",
              "      <td>157</td>\n",
              "      <td>26</td>\n",
              "      <td>134</td>\n",
              "      <td>36.1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.56</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>82</td>\n",
              "      <td>23.582766</td>\n",
              "      <td>0</td>\n",
              "      <td>157.50</td>\n",
              "      <td>58.5</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>32.8</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>25</td>\n",
              "      <td>142</td>\n",
              "      <td>36.1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>61</td>\n",
              "      <td>31.684520</td>\n",
              "      <td>1</td>\n",
              "      <td>172.70</td>\n",
              "      <td>94.5</td>\n",
              "      <td>0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.11</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>35.3</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>6</td>\n",
              "      <td>131</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0</td>\n",
              "      <td>8.59</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58</td>\n",
              "      <td>45.156250</td>\n",
              "      <td>0</td>\n",
              "      <td>160.00</td>\n",
              "      <td>115.6</td>\n",
              "      <td>0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>30.1</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>23</td>\n",
              "      <td>138</td>\n",
              "      <td>34.9</td>\n",
              "      <td>1</td>\n",
              "      <td>16.03</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74</td>\n",
              "      <td>25.817016</td>\n",
              "      <td>1</td>\n",
              "      <td>172.70</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.93</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>12</td>\n",
              "      <td>135</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0</td>\n",
              "      <td>45.80</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>73</td>\n",
              "      <td>17.943584</td>\n",
              "      <td>0</td>\n",
              "      <td>157.48</td>\n",
              "      <td>44.5</td>\n",
              "      <td>0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>33.8</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>9</td>\n",
              "      <td>144</td>\n",
              "      <td>36.9</td>\n",
              "      <td>0</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>79</td>\n",
              "      <td>29.049732</td>\n",
              "      <td>1</td>\n",
              "      <td>167.60</td>\n",
              "      <td>81.6</td>\n",
              "      <td>0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>42.7</td>\n",
              "      <td>0</td>\n",
              "      <td>163</td>\n",
              "      <td>9</td>\n",
              "      <td>139</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0</td>\n",
              "      <td>10.77</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>85</td>\n",
              "      <td>24.627827</td>\n",
              "      <td>0</td>\n",
              "      <td>152.40</td>\n",
              "      <td>57.2</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.48</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>29.5</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>9</td>\n",
              "      <td>139</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0</td>\n",
              "      <td>7.35</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>68</td>\n",
              "      <td>32.510940</td>\n",
              "      <td>1</td>\n",
              "      <td>193.00</td>\n",
              "      <td>121.1</td>\n",
              "      <td>0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.64</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>37.5</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>10</td>\n",
              "      <td>140</td>\n",
              "      <td>36.9</td>\n",
              "      <td>1</td>\n",
              "      <td>16.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>839</th>\n",
              "      <td>48</td>\n",
              "      <td>24.106828</td>\n",
              "      <td>0</td>\n",
              "      <td>157.50</td>\n",
              "      <td>59.8</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.33</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>32.3</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>14</td>\n",
              "      <td>139</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>840 rows  24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  arf_apache  bun_apache  \\\n",
              "0     62  32.866392       1  177.80   103.9           1        31.0   \n",
              "1     82  23.582766       0  157.50    58.5           0        26.0   \n",
              "2     61  31.684520       1  172.70    94.5           0        16.0   \n",
              "3     58  45.156250       0  160.00   115.6           0        19.0   \n",
              "4     74  25.817016       1  172.70    77.0           0        25.0   \n",
              "..   ...        ...     ...     ...     ...         ...         ...   \n",
              "835   73  17.943584       0  157.48    44.5           0        12.0   \n",
              "836   79  29.049732       1  167.60    81.6           0        48.0   \n",
              "837   85  24.627827       0  152.40    57.2           0        11.0   \n",
              "838   68  32.510940       1  193.00   121.1           0        14.0   \n",
              "839   48  24.106828       0  157.50    59.8           0         7.0   \n",
              "\n",
              "     creatinine_apache  gcs_eyes_apache  gcs_motor_apache  ...  \\\n",
              "0                10.30                4                 6  ...   \n",
              "1                 0.54                3                 4  ...   \n",
              "2                 1.11                4                 6  ...   \n",
              "3                 0.70                1                 4  ...   \n",
              "4                 0.93                4                 6  ...   \n",
              "..                 ...              ...               ...  ...   \n",
              "835               0.30                4                 6  ...   \n",
              "836               2.19                4                 6  ...   \n",
              "837               0.48                3                 5  ...   \n",
              "838               0.64                4                 6  ...   \n",
              "839               0.33                4                 6  ...   \n",
              "\n",
              "     hematocrit_apache  intubated_apache  map_apache  resprate_apache  \\\n",
              "0                 36.4                 0         157               26   \n",
              "1                 32.8                 0          42               25   \n",
              "2                 35.3                 0         129                6   \n",
              "3                 30.1                 1         131               23   \n",
              "4                 34.5                 0          55               12   \n",
              "..                 ...               ...         ...              ...   \n",
              "835               33.8                 0         129                9   \n",
              "836               42.7                 0         163                9   \n",
              "837               29.5                 0          67                9   \n",
              "838               37.5                 0          61               10   \n",
              "839               32.3                 0         111               14   \n",
              "\n",
              "     sodium_apache  temp_apache  ventilated_apache  wbc_apache  \\\n",
              "0              134         36.1                  0        4.56   \n",
              "1              142         36.1                  0        6.00   \n",
              "2              131         36.8                  0        8.59   \n",
              "3              138         34.9                  1       16.03   \n",
              "4              135         36.3                  0       45.80   \n",
              "..             ...          ...                ...         ...   \n",
              "835            144         36.9                  0        7.70   \n",
              "836            139         36.4                  0       10.77   \n",
              "837            139         36.6                  0        7.35   \n",
              "838            140         36.9                  1       16.02   \n",
              "839            139         36.3                  0        2.20   \n",
              "\n",
              "     apache_4a_hospital_death_prob  apache_4a_icu_death_prob  \n",
              "0                             0.06                      0.03  \n",
              "1                             0.14                      0.06  \n",
              "2                             0.05                      0.03  \n",
              "3                             0.33                      0.22  \n",
              "4                             0.12                      0.05  \n",
              "..                             ...                       ...  \n",
              "835                           0.02                      0.01  \n",
              "836                           0.06                      0.03  \n",
              "837                           0.16                      0.05  \n",
              "838                           0.00                      0.00  \n",
              "839                           0.01                      0.00  \n",
              "\n",
              "[840 rows x 24 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test = pd.read_csv('input_test.csv')\n",
        "x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH-0DxyR9qWn"
      },
      "source": [
        "## Step3 : Build a Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LD8ndJ8ymzG3"
      },
      "outputs": [],
      "source": [
        "# Define the attributes\n",
        "\n",
        "max_depth = 7\n",
        "depth = 0\n",
        "min_samples_split = 4\n",
        "\n",
        "# total number of trees in a random forest\n",
        "n_trees = 250\n",
        "\n",
        "# number of features to train a decision tree\n",
        "n_features = 5\n",
        "\n",
        "# the ratio to select the number of instances\n",
        "sample_size = 0.6\n",
        "n_samples = int(training_data.shape[0] * sample_size)\n",
        "\n",
        "mode = \"advanced\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hVl66f1aU36-"
      },
      "outputs": [],
      "source": [
        "def build_forest(data, n_trees, n_features, n_samples):\n",
        "  \"\"\"\n",
        "  This function will build a random forest.\n",
        "  args:\n",
        "  * data: all data that can be used to train a random forest\n",
        "  * n_trees: total number of tree\n",
        "  * n_features: number of features\n",
        "  * n_samples: number of instances\n",
        "  return:\n",
        "  * forest: a random forest with 'n_trees' of decision tree\n",
        "  \"\"\"\n",
        "\n",
        "  forest = []\n",
        "  for i in range(n_trees):\n",
        "\n",
        "    bootstrap_sample = data.sample(n = n_samples, replace = True)\n",
        "\n",
        "    tree = build_tree(bootstrap_sample, max_depth, min_samples_split, depth)\n",
        "    forest.append(tree)\n",
        "\n",
        "  return forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zylo6C51m3OJ"
      },
      "outputs": [],
      "source": [
        "forest = build_forest(training_data, n_trees, n_features, n_samples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dZb6EEYnnO05"
      },
      "source": [
        "## Step4 : Make predictions with the random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UbHMZnMDnWpG"
      },
      "outputs": [],
      "source": [
        "def majority_vote(values):\n",
        "  p = values.count(1)\n",
        "  n = values.count(0)\n",
        "  return 1 if p >= n else 0\n",
        "\n",
        "def make_prediction_forest(forest, data):\n",
        "  \"\"\"\n",
        "  This function will use the pre-trained random forest to make the predictions\n",
        "  args:\n",
        "  * forest: the random forest\n",
        "  * data: the data used to predict\n",
        "  return:\n",
        "  * y_prediction: the predicted results\n",
        "  \"\"\"\n",
        "\n",
        "  idx = 1\n",
        "  tree_results = []\n",
        "  for tree in forest:\n",
        "    results = []\n",
        "    for i in range(len(data)):\n",
        "      results.append(classify_data(data.iloc[i], tree))\n",
        "    tree_results.append(results)\n",
        "    if data_type == \"validation\":\n",
        "      print(\"f1-score of tree\", idx, \"=\", calculate_score(y_validation, results))\n",
        "    idx += 1\n",
        "    \n",
        "  y_prediction = []\n",
        "  for results in zip(*tree_results):\n",
        "    label = majority_vote(results)\n",
        "    y_prediction.append(label)\n",
        "  \n",
        "  if data_type == \"validation\":\n",
        "    return calculate_score(y_validation, y_prediction)\n",
        "\n",
        "  return y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Hcd70ubwgHq4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1-score of tree 1 = 0.651186790505676\n",
            "f1-score of tree 2 = 0.654282765737874\n",
            "f1-score of tree 3 = 0.678627968337731\n",
            "f1-score of tree 4 = 0.6504653567735265\n",
            "f1-score of tree 5 = 0.6338329764453962\n",
            "f1-score of tree 6 = 0.6363131593559133\n",
            "f1-score of tree 7 = 0.669710806697108\n",
            "f1-score of tree 8 = 0.6342003320420586\n",
            "f1-score of tree 9 = 0.685404339250493\n",
            "f1-score of tree 10 = 0.6070252469813392\n",
            "f1-score of tree 11 = 0.6912350597609561\n",
            "f1-score of tree 12 = 0.612534059945504\n",
            "f1-score of tree 13 = 0.669367088607595\n",
            "f1-score of tree 14 = 0.6688034188034188\n",
            "f1-score of tree 15 = 0.6492039034411915\n",
            "f1-score of tree 16 = 0.6506614404703576\n",
            "f1-score of tree 17 = 0.5756373937677054\n",
            "f1-score of tree 18 = 0.64390756302521\n",
            "f1-score of tree 19 = 0.6537842190016104\n",
            "f1-score of tree 20 = 0.6656519533231862\n",
            "f1-score of tree 21 = 0.6016172506738544\n",
            "f1-score of tree 22 = 0.6316377864728898\n",
            "f1-score of tree 23 = 0.665631469979296\n",
            "f1-score of tree 24 = 0.6556224899598394\n",
            "f1-score of tree 25 = 0.6766374419804022\n",
            "f1-score of tree 26 = 0.6599469496021221\n",
            "f1-score of tree 27 = 0.6638297872340425\n",
            "f1-score of tree 28 = 0.6551197147223636\n",
            "f1-score of tree 29 = 0.6301369863013699\n",
            "f1-score of tree 30 = 0.6371967654986523\n",
            "f1-score of tree 31 = 0.6820276497695853\n",
            "f1-score of tree 32 = 0.6178067318132465\n",
            "f1-score of tree 33 = 0.6803234501347708\n",
            "f1-score of tree 34 = 0.6292016806722689\n",
            "f1-score of tree 35 = 0.6998535871156661\n",
            "f1-score of tree 36 = 0.6510067114093959\n",
            "f1-score of tree 37 = 0.6719325961032122\n",
            "f1-score of tree 38 = 0.6646433990895296\n",
            "f1-score of tree 39 = 0.60078607523863\n",
            "f1-score of tree 40 = 0.6327433628318584\n",
            "f1-score of tree 41 = 0.6494845360824741\n",
            "f1-score of tree 42 = 0.6680851063829787\n",
            "f1-score of tree 43 = 0.6384823848238482\n",
            "f1-score of tree 44 = 0.6614404703576678\n",
            "f1-score of tree 45 = 0.655609756097561\n",
            "f1-score of tree 46 = 0.6686960933536276\n",
            "f1-score of tree 47 = 0.6259113853056647\n",
            "f1-score of tree 48 = 0.594847775175644\n",
            "f1-score of tree 49 = 0.6700404858299595\n",
            "f1-score of tree 50 = 0.6842366981601192\n",
            "f1-score of tree 51 = 0.680968096809681\n",
            "f1-score of tree 52 = 0.6609914872308463\n",
            "f1-score of tree 53 = 0.6676557863501484\n",
            "f1-score of tree 54 = 0.6469945355191257\n",
            "f1-score of tree 55 = 0.6514830508474577\n",
            "f1-score of tree 56 = 0.6585241730279898\n",
            "f1-score of tree 57 = 0.655701754385965\n",
            "f1-score of tree 58 = 0.6272439281942978\n",
            "f1-score of tree 59 = 0.5820271682340649\n",
            "f1-score of tree 60 = 0.6778711484593837\n",
            "f1-score of tree 61 = 0.6579345088161209\n",
            "f1-score of tree 62 = 0.6742966142107774\n",
            "f1-score of tree 63 = 0.5809628008752735\n",
            "f1-score of tree 64 = 0.6440501043841336\n",
            "f1-score of tree 65 = 0.6639839034205232\n",
            "f1-score of tree 66 = 0.6559251559251559\n",
            "f1-score of tree 67 = 0.6715758468335787\n",
            "f1-score of tree 68 = 0.6095340117836102\n",
            "f1-score of tree 69 = 0.6488794023479189\n",
            "f1-score of tree 70 = 0.6791497975708501\n",
            "f1-score of tree 71 = 0.6721060683324835\n",
            "f1-score of tree 72 = 0.7011669203450025\n",
            "f1-score of tree 73 = 0.6728395061728396\n",
            "f1-score of tree 74 = 0.5894621295279913\n",
            "f1-score of tree 75 = 0.6710593005575266\n",
            "f1-score of tree 76 = 0.6705756929637527\n",
            "f1-score of tree 77 = 0.6462513199577613\n",
            "f1-score of tree 78 = 0.5644098262432594\n",
            "f1-score of tree 79 = 0.67859021567596\n",
            "f1-score of tree 80 = 0.6199898528665652\n",
            "f1-score of tree 81 = 0.6716716716716717\n",
            "f1-score of tree 82 = 0.6227678571428571\n",
            "f1-score of tree 83 = 0.6872623574144487\n",
            "f1-score of tree 84 = 0.6669902912621358\n",
            "f1-score of tree 85 = 0.6324786324786326\n",
            "f1-score of tree 86 = 0.6348873755893137\n",
            "f1-score of tree 87 = 0.6663428848955805\n",
            "f1-score of tree 88 = 0.685370741482966\n",
            "f1-score of tree 89 = 0.6751918158567775\n",
            "f1-score of tree 90 = 0.6469622331691298\n",
            "f1-score of tree 91 = 0.5917901938426454\n",
            "f1-score of tree 92 = 0.6473214285714286\n",
            "f1-score of tree 93 = 0.6634615384615384\n",
            "f1-score of tree 94 = 0.6566566566566566\n",
            "f1-score of tree 95 = 0.6530825496342738\n",
            "f1-score of tree 96 = 0.6703910614525139\n",
            "f1-score of tree 97 = 0.6906614785992218\n",
            "f1-score of tree 98 = 0.6939942802669209\n",
            "f1-score of tree 99 = 0.6266050333846944\n",
            "f1-score of tree 100 = 0.6082004555808657\n",
            "f1-score of tree 101 = 0.6695005313496281\n",
            "f1-score of tree 102 = 0.6226053639846744\n",
            "f1-score of tree 103 = 0.6812652068126521\n",
            "f1-score of tree 104 = 0.648910411622276\n",
            "f1-score of tree 105 = 0.630879911455451\n",
            "f1-score of tree 106 = 0.6808510638297872\n",
            "f1-score of tree 107 = 0.6745182012847964\n",
            "f1-score of tree 108 = 0.6683143845773604\n",
            "f1-score of tree 109 = 0.6359338061465721\n",
            "f1-score of tree 110 = 0.6764545948885264\n",
            "f1-score of tree 111 = 0.6083565459610027\n",
            "f1-score of tree 112 = 0.6766766766766766\n",
            "f1-score of tree 113 = 0.5935412026726058\n",
            "f1-score of tree 114 = 0.6892768079800499\n",
            "f1-score of tree 115 = 0.6281833616298811\n",
            "f1-score of tree 116 = 0.6684182869153967\n",
            "f1-score of tree 117 = 0.6617257808364213\n",
            "f1-score of tree 118 = 0.5993520518358532\n",
            "f1-score of tree 119 = 0.6712871287128713\n",
            "f1-score of tree 120 = 0.6256742179072275\n",
            "f1-score of tree 121 = 0.6513527309851965\n",
            "f1-score of tree 122 = 0.6421267893660533\n",
            "f1-score of tree 123 = 0.6169312169312169\n",
            "f1-score of tree 124 = 0.6062906724511932\n",
            "f1-score of tree 125 = 0.6218487394957982\n",
            "f1-score of tree 126 = 0.6540540540540541\n",
            "f1-score of tree 127 = 0.6220595922634605\n",
            "f1-score of tree 128 = 0.6540567446490791\n",
            "f1-score of tree 129 = 0.6246514221974344\n",
            "f1-score of tree 130 = 0.6837160751565763\n",
            "f1-score of tree 131 = 0.6496083550913838\n",
            "f1-score of tree 132 = 0.6688000000000001\n",
            "f1-score of tree 133 = 0.660347551342812\n",
            "f1-score of tree 134 = 0.6282327586206896\n",
            "f1-score of tree 135 = 0.6218487394957982\n",
            "f1-score of tree 136 = 0.6565217391304348\n",
            "f1-score of tree 137 = 0.6701791359325606\n",
            "f1-score of tree 138 = 0.6128500823723229\n",
            "f1-score of tree 139 = 0.6048257372654154\n",
            "f1-score of tree 140 = 0.6717171717171717\n",
            "f1-score of tree 141 = 0.6447728516694035\n",
            "f1-score of tree 142 = 0.6280737704918032\n",
            "f1-score of tree 143 = 0.609271523178808\n",
            "f1-score of tree 144 = 0.6498659517426273\n",
            "f1-score of tree 145 = 0.6487880350696236\n",
            "f1-score of tree 146 = 0.6449921342422652\n",
            "f1-score of tree 147 = 0.6659919028340081\n",
            "f1-score of tree 148 = 0.6771488469601677\n",
            "f1-score of tree 149 = 0.6287323943661972\n",
            "f1-score of tree 150 = 0.6695511087074094\n",
            "f1-score of tree 151 = 0.6583850931677019\n",
            "f1-score of tree 152 = 0.6504592112371692\n",
            "f1-score of tree 153 = 0.6687631027253668\n",
            "f1-score of tree 154 = 0.6212534059945504\n",
            "f1-score of tree 155 = 0.6261530113944656\n",
            "f1-score of tree 156 = 0.6239837398373984\n",
            "f1-score of tree 157 = 0.6334639059876889\n",
            "f1-score of tree 158 = 0.6155567991046447\n",
            "f1-score of tree 159 = 0.6534446764091859\n",
            "f1-score of tree 160 = 0.6460759493670887\n",
            "f1-score of tree 161 = 0.6304464766003228\n",
            "f1-score of tree 162 = 0.6415678184631254\n",
            "f1-score of tree 163 = 0.6564885496183206\n",
            "f1-score of tree 164 = 0.6633216256899147\n",
            "f1-score of tree 165 = 0.660609299839658\n",
            "f1-score of tree 166 = 0.685858585858586\n",
            "f1-score of tree 167 = 0.6679900744416872\n",
            "f1-score of tree 168 = 0.6591880341880343\n",
            "f1-score of tree 169 = 0.5804046858359957\n",
            "f1-score of tree 170 = 0.6365546218487395\n",
            "f1-score of tree 171 = 0.697220867869332\n",
            "f1-score of tree 172 = 0.6102088167053364\n",
            "f1-score of tree 173 = 0.6018746916625555\n",
            "f1-score of tree 174 = 0.5943502824858757\n",
            "f1-score of tree 175 = 0.5928074245939676\n",
            "f1-score of tree 176 = 0.622546270330903\n",
            "f1-score of tree 177 = 0.5653753026634382\n",
            "f1-score of tree 178 = 0.6684723726977247\n",
            "f1-score of tree 179 = 0.655958549222798\n",
            "f1-score of tree 180 = 0.6442307692307693\n",
            "f1-score of tree 181 = 0.6340179041600842\n",
            "f1-score of tree 182 = 0.6712328767123288\n",
            "f1-score of tree 183 = 0.6847770374167093\n",
            "f1-score of tree 184 = 0.6518196632265073\n",
            "f1-score of tree 185 = 0.6763185108583247\n",
            "f1-score of tree 186 = 0.6964824120603015\n",
            "f1-score of tree 187 = 0.6716566866267465\n",
            "f1-score of tree 188 = 0.6422645192776966\n",
            "f1-score of tree 189 = 0.672216441207076\n",
            "f1-score of tree 190 = 0.6578171091445428\n",
            "f1-score of tree 191 = 0.6258064516129033\n",
            "f1-score of tree 192 = 0.6358059118795315\n",
            "f1-score of tree 193 = 0.6590114827758361\n",
            "f1-score of tree 194 = 0.7059401080019637\n",
            "f1-score of tree 195 = 0.6544190665342602\n",
            "f1-score of tree 196 = 0.5569917743830787\n",
            "f1-score of tree 197 = 0.5979955456570156\n",
            "f1-score of tree 198 = 0.6095534787123571\n",
            "f1-score of tree 199 = 0.6239495798319328\n",
            "f1-score of tree 200 = 0.5909345271404589\n",
            "f1-score of tree 201 = 0.6826042726347914\n",
            "f1-score of tree 202 = 0.6267529665587918\n",
            "f1-score of tree 203 = 0.6959459459459458\n",
            "f1-score of tree 204 = 0.6670087224217548\n",
            "f1-score of tree 205 = 0.6264150943396226\n",
            "f1-score of tree 206 = 0.6291502532357905\n",
            "f1-score of tree 207 = 0.6745308310991958\n",
            "f1-score of tree 208 = 0.6524064171122994\n",
            "f1-score of tree 209 = 0.6470908102229472\n",
            "f1-score of tree 210 = 0.6089238845144357\n",
            "f1-score of tree 211 = 0.5714285714285714\n",
            "f1-score of tree 212 = 0.674905964535196\n",
            "f1-score of tree 213 = 0.6134715025906735\n",
            "f1-score of tree 214 = 0.6742385131646876\n",
            "f1-score of tree 215 = 0.6959921798631477\n",
            "f1-score of tree 216 = 0.6094929881337648\n",
            "f1-score of tree 217 = 0.6206896551724138\n",
            "f1-score of tree 218 = 0.6535733769776323\n",
            "f1-score of tree 219 = 0.6308724832214765\n",
            "f1-score of tree 220 = 0.6277620396600566\n",
            "f1-score of tree 221 = 0.6504618376276131\n",
            "f1-score of tree 222 = 0.6578108395324124\n",
            "f1-score of tree 223 = 0.6427532719340766\n",
            "f1-score of tree 224 = 0.6440854611776967\n",
            "f1-score of tree 225 = 0.6735430634347601\n",
            "f1-score of tree 226 = 0.6634563937934724\n",
            "f1-score of tree 227 = 0.6614437604924455\n",
            "f1-score of tree 228 = 0.6142241379310345\n",
            "f1-score of tree 229 = 0.6840039254170756\n",
            "f1-score of tree 230 = 0.6562662506500261\n",
            "f1-score of tree 231 = 0.7017031630170317\n",
            "f1-score of tree 232 = 0.6858513189448442\n",
            "f1-score of tree 233 = 0.6512378902045209\n",
            "f1-score of tree 234 = 0.5910614525139665\n",
            "f1-score of tree 235 = 0.6830738322451029\n",
            "f1-score of tree 236 = 0.6350559403303142\n",
            "f1-score of tree 237 = 0.6395778364116095\n",
            "f1-score of tree 238 = 0.6701030927835051\n",
            "f1-score of tree 239 = 0.6321585903083701\n",
            "f1-score of tree 240 = 0.7020148462354189\n",
            "f1-score of tree 241 = 0.6591276252019387\n",
            "f1-score of tree 242 = 0.6646341463414634\n",
            "f1-score of tree 243 = 0.657074340527578\n",
            "f1-score of tree 244 = 0.6311868014901544\n",
            "f1-score of tree 245 = 0.6860404774260508\n",
            "f1-score of tree 246 = 0.6687370600414078\n",
            "f1-score of tree 247 = 0.6178489702517163\n",
            "f1-score of tree 248 = 0.6174496644295302\n",
            "f1-score of tree 249 = 0.6479877738155884\n",
            "f1-score of tree 250 = 0.6184649610678532\n",
            "Final f1-score = 0.7174139728884255\n"
          ]
        }
      ],
      "source": [
        "data_type = \"validation\"\n",
        "print(\"Final f1-score =\", make_prediction_forest(forest, x_validation))\n",
        "data_type = \"test\"\n",
        "y_pred_test = make_prediction_forest(forest, x_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "23b4a3e8622309bcc6db3d5cc6eb73d60ab98d9ec23bad6a26b709981ccb403a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
